# 作为 API 服务运行 DeepSeek-R1 模型

## 简介

KTransformers 提供了一个用于启动 API 服务的命令行脚本。

本章节内容适用于“v0.2.3”版本。从“v0.2.4”版本开始，部分参数有所调整，详细信息请参考后面的章节。

参数说明文档：https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/server/backend/args.py

## 使用虚拟环境

1. 使用虚拟环境：
```
conda activate ktransformers
```

## 下载模型文件

参考前面的章节。

## 修改分词器配置文件

1. 编辑分词器配置文件：
```
cd /app/data/model/DeepSeek-R1-GGUF/

vim tokenizer_config.json
```

2. 找到以下内容：
```
  "model_max_length": 16384,
```

3. 将“model_max_length”参数的值从“16384”修改成“131072”。

## 复制优化规则文件

1. 复制优化规则文件：
```
mkdir -p /app/data/ktransformers/optimize_rules

cp \
    /app/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml \
    /app/data/ktransformers/optimize_rules/DeepSeek-R1-Chat.yaml
```

## 修改优化规则文件

1. 编辑优化规则文件：
```
cd /app/data/ktransformers/optimize_rules/

vim DeepSeek-R1-Chat.yaml
```

2. 找到以下内容：
```
- match:
    name: "^model\\.layers\\..*\\.self_attn$"
  replace:
    class: ktransformers.operators.attention.KDeepseekV2Attention # optimized MLA implementation
    kwargs:
      generate_device: "cuda"
      prefill_device: "cuda"
      absorb_for_prefill: False # change this to True to enable long context(prefill may slower).
```

3. 将“absorb_for_prefill”参数的值从“False”修改成“True”。

## 作为服务运行

1. 作为服务运行模型，执行以下命令：
```
ktransformers \
    --model_path /app/data/model/DeepSeek-R1-GGUF \
    --gguf_path /app/data/model/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M \
    --optimize_config_path /app/data/ktransformers/optimize_rules/DeepSeek-R1-Chat.yaml \
    --model_name deepseek-r1 \
    --host 0.0.0.0 \
    --port 10002 \
    --api_key 123 \
    --cpu_infer 30 \
    --force_think \
    --total_context 131072 \
    --cache_lens 131072 \
    --length 131072 \
    --max_new_tokens 131072 \
    --paged true \
    --chunk_prefill_size 2048
```

参数说明：
* --model_path：包含模型配置文件的目录路径。
* --gguf_path：包含模型权重文件的目录路径。
* --optimize_config_path：优化规则文件的路径。
* --model_name：模型名称。
* --host：允许访问服务的网络地址，允许局域网访问需要设置成“0.0.0.0”。
* --port：服务监听的端口号。
* --api_key：服务的访问凭证。
* --cpu_infer：推理过程中使用的 CPU 核心数量。
* --force_think：强制模型思考。
* --total_context：模型可以使用的最大 token 数量，所有会话的总量。
* --cache_lens：模型可以缓存的最大 token 数量，所有会话的总量。
* --length：模型的上下文长度，模型在一次推理过程中，可以处理的最大 token 数量。
* --max_new_tokens：模型在一次推理过程中，可以生成的最大 token 数量，超过这个数量时，生成过程会被强制停止。
* --paged：是否启用分页注意力机制，这是一种优化长序列推理的方法，启用后可以减少显存占用。
* --chunk_prefill_size：分块预填充大小，指定预填充（prefill）阶段的分块大小，即每次加载多少 token 进行计算。

等待几分钟后，出现内容：
```
INFO:     Started server process [66666]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:10002 (Press CTRL+C to quit)
```

2. 使用前端程序添加模型服务：
* API 地址：http://0.0.0.0:10002（将“0.0.0.0”替换为你 Ubuntu 系统的实际 IP 地址）
* API 密钥：123
* 模型 ID：deepseek-r1
* 模型类型：推理、工具

3. 使用前端程序发送以下内容：
```
你好
```

等待几秒钟后，出现内容：
```
你好！有什么可以帮助您的吗？
```

4. 查看 shell 会话，出现内容：
```
<think>
好的，用户问候我，我需要回应。首先，保持礼貌和亲切的语气。用中文回复，因为用户使用的是中文。可能需要询问用户有什么需要帮助的地方，或者直接提供帮助。例如：“你好！有什么可以帮助您的吗？”或者更简洁的回应：“你好！请问有什么需要帮忙的吗？”确保回复友好且专业，同时简洁明了。
</think>

你好！有什么可以帮助您的吗？
2025-05-09 05:56:29,375 INFO /app/anaconda3/envs/ktransformers/lib/python3.12/site-packages/ktransformers/server/backend/base.py[65]: Performance(T/s): prefill 3.7692156304171807, decode 8.538006887782355. Time(s): tokenize 0.009102344512939453, prefill 0.2653071880340576, decode 9.48699164390564
```

## 运行配置文件的存放目录

1. 查看目录内容：
```
ls -l ~/.ktransformers
```

出现内容：
```
-rw-rw-r-- 1 uu uu 963  5月  9 05:51 config.yaml
```

2. 删除目录：
```
rm -rf ~/.ktransformers
```

【注意】：运行不同版本的 KTransformers 程序前，需要删除 `.ktransformers` 目录，因为不同版本的 KTransformers 使用的运行配置可能不兼容，如果不删除旧的运行配置文件，可能导致程序启动失败。

