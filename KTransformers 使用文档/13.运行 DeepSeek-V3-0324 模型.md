# 运行 DeepSeek-V3-0324 模型

## 简介

原始模型地址：https://modelscope.cn/models/deepseek-ai/DeepSeek-V3-0324
量化模型地址：https://modelscope.cn/models/unsloth/DeepSeek-V3-0324-GGUF

## 使用虚拟环境

1. 使用虚拟环境：
```
conda activate ktransformers
```

## 下载模型文件

1. 下载 DeepSeek-V3-0324 模型的配置文件：
```
modelscope download \
    --model deepseek-ai/DeepSeek-V3-0324 \
        tokenizer.json tokenizer_config.json \
        configuration_deepseek.py modeling_deepseek.py \
    --local_dir /app/data/model/DeepSeek-V3-0324-GGUF
````

2. 下载 DeepSeek-V3-0324-GGUF 模型的配置文件：
```
modelscope download \
    --model unsloth/DeepSeek-V3-0324-GGUF \
        config.json configuration.json \
    --local_dir /app/data/model/DeepSeek-V3-0324-GGUF
```

3. 下载 DeepSeek-V3-0324-GGUF 模型的权重文件：
```
modelscope download \
    --model unsloth/DeepSeek-V3-0324-GGUF \
    --include Q4_K_M/DeepSeek-V3-0324-Q4_K_M-*.gguf \
    --local_dir /app/data/model/DeepSeek-V3-0324-GGUF
```

## 复制优化规则文件

1. 复制优化规则文件：
```
mkdir -p /app/data/ktransformers/optimize_rules

cp \
    /app/ktransformers/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat.yaml \
    /app/data/ktransformers/optimize_rules/DeepSeek-V3-0324-Chat.yaml
```

## 修改优化规则文件

1. 编辑优化规则文件：
```
cd /app/data/ktransformers/optimize_rules/

vim DeepSeek-V3-0324-Chat.yaml
```

2. 找到以下内容：
```
- match:
    name: "^model\\.layers\\..*\\.self_attn$"
  replace:
    class: ktransformers.operators.attention.KDeepseekV2Attention # optimized MLA implementation
    kwargs:
      generate_device: "cuda"
      prefill_device: "cuda"
      absorb_for_prefill: False # change this to True to enable long context(prefill may slower).
```

3. 将“absorb_for_prefill”参数的值从“False”修改成“True”。

## 作为服务运行

1. 作为服务运行模型，执行以下命令：
```
ktransformers \
    --model_path /app/data/model/DeepSeek-V3-0324-GGUF \
    --gguf_path /app/data/model/DeepSeek-V3-0324-GGUF/Q4_K_M \
    --optimize_config_path /app/data/ktransformers/optimize_rules/DeepSeek-V3-0324-Chat.yaml \
    --model_name deepseek-v3 \
    --host 0.0.0.0 \
    --port 10002 \
    --api_key 123 \
    --cpu_infer 30 \
    --force_think \
    --total_context 131072 \
    --cache_lens 131072 \
    --length 131072 \
    --max_new_tokens 131072 \
    --paged true \
    --chunk_prefill_size 2048
```

