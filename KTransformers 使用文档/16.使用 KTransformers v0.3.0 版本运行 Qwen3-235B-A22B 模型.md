# 使用 KTransformers v0.3.0 版本运行 Qwen3-235B-A22B 模型

## 简介

原始模型库地址：https://www.modelscope.cn/models/Qwen/Qwen3-235B-A22B
量化模型库地址：https://www.modelscope.cn/models/unsloth/Qwen3-235B-A22B-128K-GGUF

## 使用虚拟环境

1. 使用虚拟环境：
```
conda activate ktransformers30
```

## 下载模型文件

1. 下载 Qwen3-235B-A22B 模型的配置文件：
```
modelscope download \
    --model Qwen/Qwen3-235B-A22B \
        generation_config.json \
        merges.txt \
        tokenizer.json tokenizer_config.json \
        vocab.json \
    --local_dir /app/data/model/Qwen3-235B-A22B-128K-GGUF
```

2. 下载 Qwen3-235B-A22B-128K-GGUF 模型的配置文件：
```
modelscope download \
    --model unsloth/Qwen3-235B-A22B-128K-GGUF \
        config.json configuration.json \
    --local_dir /app/data/model/Qwen3-235B-A22B-128K-GGUF
```

3. 下载 Qwen3-235B-A22B-128K-GGUF 模型的权重文件：
```
modelscope download \
    --model unsloth/Qwen3-235B-A22B-128K-GGUF \
    --include Q4_K_M/Qwen3-235B-A22B-128K-Q4_K_M-*.gguf \
    --local_dir /app/data/model/Qwen3-235B-A22B-128K-GGUF
```

## 复制优化规则文件

1. 复制优化规则文件：
```
mkdir -p /app/data/ktransformers/optimize_rules

cp \
    /app/ktransformers30/ktransformers/optimize/optimize_rules/Qwen3Moe-serve.yaml \
    /app/data/ktransformers/optimize_rules/Qwen3Moe-serve.yaml
```

## 作为服务运行
1. 作为服务运行：
```
ktransformers \
    --model_path /app/data/model/Qwen3-235B-A22B-128K-GGUF \
    --gguf_path /app/data/model/Qwen3-235B-A22B-128K-GGUF/Q4_K_M \
    --optimize_config_path /app/data/ktransformers/optimize_rules/Qwen3Moe-serve.yaml \
    --architectures Qwen3MoeForCausalLM \
    --model_name qwen3 \
    --host 0.0.0.0 \
    --port 10002 \
    --api_key 123 \
    --cpu_infer 30 \
    --force_think \
    --cache_lens 92160 \
    --length 92160 \
    --max_new_tokens 46080 \
    --chunk_size 256 \
    --max_batch_size 4 \
    --backend_type balance_serve
```

等待几分钟后，出现内容：
```
INFO:     Started server process [66666]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:10002 (Press CTRL+C to quit)
```

