# 使用 KTransformers v0.3.0 版本运行 DeepSeek-R1 模型

## 简介

原始模型库地址：https://modelscope.cn/models/deepseek-ai/DeepSeek-R1
量化模型库地址：https://modelscope.cn/models/unsloth/DeepSeek-R1-GGUF

## 使用虚拟环境

1. 使用虚拟环境：
```
conda activate ktransformers30
```

## 下载模型文件

参考前面的章节。

## 复制优化规则文件

1. 复制优化规则文件：
```
mkdir -p /app/data/ktransformers/optimize_rules

cp \
    /app/ktransformers30/ktransformers/optimize/optimize_rules/DeepSeek-V3-Chat-serve.yaml \
    /app/data/ktransformers/optimize_rules/DeepSeek-R1-Chat-serve.yaml
```

## 作为服务运行

1. 作为服务运行模型，执行以下命令：
```
ktransformers \
    --model_path /app/data/model/DeepSeek-R1-GGUF \
    --gguf_path /app/data/model/DeepSeek-R1-GGUF/DeepSeek-R1-Q4_K_M \
    --optimize_config_path /app/data/ktransformers/optimize_rules/DeepSeek-R1-Chat-serve.yaml \
    --model_name deepseek-r1 \
    --host 0.0.0.0 \
    --port 10002 \
    --api_key 123 \
    --cpu_infer 30 \
    --force_think \
    --cache_lens 131072 \
    --length 131072 \
    --max_new_tokens 40960 \
    --chunk_size 256 \
    --max_batch_size 4 \
    --backend_type balance_serve
```

参数说明：
* --model_path：包含模型配置文件的目录路径。
* --gguf_path：包含模型权重文件的目录路径。
* --optimize_config_path：优化规则文件的路径。
* --model_name：模型名称。
* --host：允许访问服务的网络地址，允许局域网访问需要设置成 0.0.0.0。
* --port：服务监听的端口号。
* --api_key：服务的访问凭证。
* --cpu_infer：推理过程中使用的 CPU 核心数量。
* --force_think：强制模型思考。
* --cache_lens：模型可以缓存的最大 token 数量，所有会话的总量。
* --length：模型的上下文长度，模型在一次推理过程中，可以处理的最大 token 数量。
* --max_new_tokens：模型在一次推理过程中，可以生成的最大 token 数量，超过这个数量时，生成过程会被强制停止。
* --chunk_size：每次推理循环中，推理引擎传入模型进行处理的最大 token 数量。
* --max_batch_size：推理引擎可以并发处理的最大会话数量。（仅 balance_serve 引擎支持。）
* --backend_type：后端推理引擎。（balance_serve 是从“v0.2.4”版本开始引入的多并发后端推理引擎。）

等待几分钟后，出现内容：
```
INFO:     Started server process [66666]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:10002 (Press CTRL+C to quit)
```

